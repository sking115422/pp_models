{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from pycocotools.coco import COCO\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "import torchvision.models.detection as detection_models\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = \"/mnt/nis_lab_research/data/coco_files/clean/far_shah-b1-b2_cln\"\n",
    "out_dir = \"../../data/obj_det/far_shah-b1-b2_cln\"\n",
    "batch_size = 8  \n",
    "num_workers = 8 \n",
    "shuffle = True\n",
    "num_classes = 27 + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coco_train_test_split(in_dir, out_dir):\n",
    "    fn = in_dir.split(\"/\")[-1]\n",
    "    \n",
    "    if fn == None:\n",
    "        fn = in_dir.split(\"/\")[-2]\n",
    "  \n",
    "    # out_dir = os.getcwd() + \"/\" + fn + \"_split\"\n",
    "    \n",
    "    if not os.path.exists(out_dir):\n",
    "        os.mkdir(out_dir)\n",
    "\n",
    "        train_dir = out_dir + \"/train\"\n",
    "        os.mkdir(train_dir)\n",
    "        train_img_dir = train_dir + \"/images\"\n",
    "        os.mkdir(train_img_dir)\n",
    "\n",
    "        test_dir = out_dir + \"/test\"\n",
    "        os.mkdir(test_dir)\n",
    "        test_img_dir = test_dir + \"/images\"\n",
    "        os.mkdir(test_img_dir)\n",
    "\n",
    "        train_split = 0.8\n",
    "\n",
    "        f = open(in_dir + \"/result.json\")\n",
    "        coco_json = json.load(f)\n",
    "        f.close()\n",
    "\n",
    "        num_img = len(coco_json[\"images\"])\n",
    "\n",
    "        img_list = coco_json[\"images\"]\n",
    "        cat_list = coco_json[\"categories\"]\n",
    "        ann_list = coco_json[\"annotations\"]\n",
    "\n",
    "        train_num = math.floor(num_img * train_split)\n",
    "\n",
    "        train_img_list = img_list[0:train_num]\n",
    "        test_img_list = img_list[train_num:]\n",
    "\n",
    "        for each in train_img_list:\n",
    "            img_name = each[\"file_name\"].split(\"/\")[-1]\n",
    "            shutil.copy(in_dir + \"/images/\" + img_name, train_img_dir + \"/\" + img_name)\n",
    "\n",
    "        for each in test_img_list:\n",
    "            img_name = each[\"file_name\"].split(\"/\")[-1]\n",
    "            shutil.copy(in_dir + \"/images/\" + img_name, test_img_dir + \"/\" + img_name)\n",
    "\n",
    "        co_val = train_img_list[-1][\"id\"]\n",
    "\n",
    "        train_ann_list = []\n",
    "        test_ann_list = []\n",
    "\n",
    "        for each in ann_list:\n",
    "            if each[\"image_id\"] <= co_val:\n",
    "                train_ann_list.append(each)\n",
    "            else:\n",
    "                test_ann_list.append(each)\n",
    "\n",
    "        train_json = {\n",
    "            \"images\": train_img_list,\n",
    "            \"categories\": cat_list,\n",
    "            \"annotations\": train_ann_list\n",
    "        }\n",
    "\n",
    "        test_json = {\n",
    "            \"images\": test_img_list,\n",
    "            \"categories\": cat_list,\n",
    "            \"annotations\": test_ann_list\n",
    "        }\n",
    "\n",
    "        train_j_out = json.dumps(train_json, indent=4)\n",
    "        test_j_out = json.dumps(test_json, indent=4)\n",
    "\n",
    "        with open(train_dir + \"/result.json\", \"w\") as outfile:\n",
    "            outfile.write(train_j_out)\n",
    "        with open(test_dir + \"/result.json\", \"w\") as outfile:\n",
    "            outfile.write(test_j_out)\n",
    "            \n",
    "        print(\"creating \" + str(train_split) + \" train test split to path: \" + out_dir)\n",
    "        \n",
    "    else:\n",
    "        print(\"directory: \" + out_dir + \" already exists!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_train_test_split(in_dir, out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoDetection(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, annFile, transform=None):\n",
    "        self.root = root\n",
    "        self.coco = COCO(annFile)\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        coco = self.coco\n",
    "        img_id = self.ids[index]\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "        annotations = coco.loadAnns(ann_ids)\n",
    "\n",
    "        path = coco.loadImgs(img_id)[0]['file_name']\n",
    "        img = Image.open(os.path.join(self.root, path)).convert('RGB')\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        img_ids = []\n",
    "        for ann in annotations:\n",
    "            # Convert COCO bbox format (x_min, y_min, width, height) to (x_min, y_min, x_max, y_max)\n",
    "            x, y, w, h = ann['bbox']\n",
    "            x_max = x + w\n",
    "            y_max = y + h\n",
    "\n",
    "            # Check if the bounding box is valid (positive width and height)\n",
    "            if w > 0 and h > 0:\n",
    "                boxes.append([x, y, x_max, y_max])\n",
    "                labels.append(ann['category_id'])\n",
    "                img_ids.append(img_id)\n",
    "\n",
    "        if len(boxes) == 0:\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels = torch.zeros((0,), dtype=torch.int64)\n",
    "            img_ids.append(img_id)\n",
    "        else:\n",
    "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "            img_ids.append(img_id)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"img_ids\"] = img_ids\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cust_coll(batch):\n",
    "\n",
    "    # Separate data and targets\n",
    "    batch = list(zip(*batch))\n",
    "\n",
    "    # Default collate for images\n",
    "    images = default_collate(batch[0])\n",
    "\n",
    "    # Targets are a list of dictionaries\n",
    "    targets = batch[1]\n",
    "\n",
    "    return images, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = CocoDetection(root=\"../../data/obj_det/far_shah-b1-b2_cln/train/images/\", \n",
    "                          annFile=\"../../data/obj_det/far_shah-b1-b2_cln/train/result.json\", \n",
    "                          transform=transform)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=shuffle, \n",
    "                           num_workers=num_workers, collate_fn=cust_coll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = CocoDetection(root=\"../../data/obj_det/far_shah-b1-b2_cln/test/images/\", \n",
    "                         annFile=\"../../data/obj_det/far_shah-b1-b2_cln/test/result.json\", \n",
    "                         transform=transform)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True, \n",
    "                         num_workers=num_workers, collate_fn=cust_coll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_classes):\n",
    "    # Load a pre-trained ResNet50 model\n",
    "    backbone = models.resnet50(pretrained=True)\n",
    "    \n",
    "    # Remove the fully connected layers (classification head)\n",
    "    # Keep only the layers up to the final convolutional layer\n",
    "    modules = list(backbone.children())[:-2]\n",
    "    backbone = torch.nn.Sequential(*modules)\n",
    "    \n",
    "    # Remove the fully connected layer (classification head)\n",
    "    # Replace the classifier head of the backbone with a new one\n",
    "    backbone.out_channels = 2048\n",
    "\n",
    "    # Create an anchor generator for the FPN which is used in Faster R-CNN\n",
    "    anchor_generator = AnchorGenerator(\n",
    "        sizes=((32,), (64,), (128,), (256,), (512,)),  # One size for each feature map\n",
    "        aspect_ratios=((0.5, 1.0, 2.0),) * 5  # Same aspect ratios for each feature map\n",
    "    )\n",
    "\n",
    "    # Define the Region Proposal Network (RPN)\n",
    "    roi_pooler = MultiScaleRoIAlign(\n",
    "        featmap_names=['0', '1', '2', '3'],\n",
    "        output_size=7,\n",
    "        sampling_ratio=2\n",
    "    )\n",
    "\n",
    "    # Create the Faster R-CNN model\n",
    "    model = FasterRCNN(backbone,\n",
    "                       num_classes=num_classes,\n",
    "                       rpn_anchor_generator=anchor_generator,\n",
    "                       box_roi_pool=roi_pooler)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = get_model(num_classes = 27+1)\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretained=True)\n",
    "num_classes = 27+1\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "for epoch in range(0, num_epochs):\n",
    "    \n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        \n",
    "        images, targets = data\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        loss_dict = model(images, targets)\n",
    "\n",
    "        # The loss is the sum of all individual losses\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        # Backward pass\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += losses.item()\n",
    "        \n",
    "        print(i, running_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the model later\n",
    "model = torch.load('./pth/far_shah-b1-b2_cln_rn50_pt_ep3.pth')\n",
    "model.eval()  # Set it to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for one in test_loader:\n",
    "    for item in one[1]:\n",
    "        print(item[\"img_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Load the COCO ground truth\n",
    "coco_path = \"../../data/obj_det/far_shah-b1-b2_cln/test/result.json\"\n",
    "cocoGt = COCO(coco_path)\n",
    "\n",
    "img_ids = []\n",
    "with open(coco_path, \"r\") as f:\n",
    "    obj = json.load(f)\n",
    "for img in obj[\"images\"]:\n",
    "    img_ids.append(img[\"id\"])\n",
    "\n",
    "\n",
    "# Prepare for COCO evaluation\n",
    "coco_results = []\n",
    "ind = 0\n",
    "with torch.no_grad():\n",
    "    for images, targets in test_loader:\n",
    "    \n",
    "        images = list(img.to(device) for img in images)\n",
    "        outputs = model(images)\n",
    "\n",
    "        for i, output in enumerate(outputs):\n",
    "            print(ind)\n",
    "            img_ids = targets[i][\"img_ids\"]\n",
    "\n",
    "            for box, label, score, img_id in zip(output[\"boxes\"], output[\"labels\"], output[\"scores\"], img_ids):\n",
    "                box = box.cpu().numpy()\n",
    "                box = [float(n) for n in box]\n",
    "                score = float(score)\n",
    "                label = int(label)\n",
    "\n",
    "                coco_result = {\n",
    "                    \"image_id\": img_id,\n",
    "                    \"category_id\": label,\n",
    "                    \"bbox\": [box[0], box[1], box[2] - box[0], box[3] - box[1]],\n",
    "                    \"score\": score\n",
    "                }\n",
    "                coco_results.append(coco_result)\n",
    "                \n",
    "            ind += 1\n",
    "\n",
    "# Save the results in a file\n",
    "with open(\"coco_results.json\", \"w\") as f:\n",
    "    json.dump(coco_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results into COCO data structure\n",
    "cocoDt = cocoGt.loadRes(\"coco_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COCO evaluation\n",
    "cocoEval = COCOeval(cocoGt, cocoDt, 'bbox')\n",
    "cocoEval.params.imgIds = img_ids\n",
    "cocoEval.evaluate()\n",
    "cocoEval.accumulate()\n",
    "cocoEval.summarize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_class_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
