{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gc\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "import random\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_curve, auc, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting CUDA devices as visible\n",
    "cuda_devices = \"0,1\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = cuda_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using GPUs:\")\n",
    "    for device_id in cuda_devices.split(','):\n",
    "        device = torch.device(f\"cuda:{device_id}\")\n",
    "        print(f\"  Device {device_id}: {torch.cuda.get_device_name(int(device_id))}\")\n",
    "else:\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classes(directory):\n",
    "    class_list = []\n",
    "    for item in os.listdir(directory):\n",
    "        item_path = os.path.join(directory, item)\n",
    "        if os.path.isdir(item_path) and os.listdir(item_path):\n",
    "            class_list.append(item)\n",
    "    return class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = \"/mnt/nis_lab_research/data/class_data/neg/far_shah_b1-b5_b8_train_neg_cln\"\n",
    "out_dir = \"../../data/classifier/far_shah_b1-b5_b8_train_neg_cln\"\n",
    "num_workers = os.cpu_count() or 32\n",
    "epochs = 25\n",
    "class_list = sorted(get_classes(in_dir))\n",
    "num_classes = len(class_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to the input size expected by ResNet\n",
    "    transforms.ToTensor(),\n",
    "    # CHANGE TO BE DATA SPECIFIC\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tt_split(input_dir, out_dir, train_ratio):\n",
    "    \"\"\"\n",
    "    Splits the dataset in the given directory into train and test sets.\n",
    "\n",
    "    :param input_dir: Path to the input directory.\n",
    "    :param train_ratio: Ratio of train set (between 0 and 1).\n",
    "    \"\"\"\n",
    "    if not 0 <= train_ratio <= 1:\n",
    "        raise ValueError(\"Train ratio must be between 0 and 1\")\n",
    "\n",
    "    base_dir = out_dir\n",
    "    train_dir = os.path.join(base_dir, 'train')\n",
    "    test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "    # Create train and test directories\n",
    "    for directory in [train_dir, test_dir]:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    # Process each class directory\n",
    "    for class_name in os.listdir(input_dir):\n",
    "        class_dir = os.path.join(input_dir, class_name)\n",
    "        if os.path.isdir(class_dir):\n",
    "            # Create class directories in train and test\n",
    "            os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)\n",
    "            os.makedirs(os.path.join(test_dir, class_name), exist_ok=True)\n",
    "\n",
    "            # Get a list of images and shuffle them\n",
    "            images = os.listdir(class_dir)\n",
    "            random.shuffle(images)\n",
    "\n",
    "            # Split images into train and test\n",
    "            split_point = int(len(images) * train_ratio)\n",
    "            train_images = images[:split_point]\n",
    "            test_images = images[split_point:]\n",
    "\n",
    "            # Copy images to train and test directories\n",
    "            for image in train_images:\n",
    "                shutil.copy2(os.path.join(class_dir, image), os.path.join(train_dir, class_name))\n",
    "            for image in test_images:\n",
    "                shutil.copy2(os.path.join(class_dir, image), os.path.join(test_dir, class_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(out_dir):\n",
    "    tt_split(in_dir, out_dir, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.ImageFolder(root='../../data/classifier/' + in_dir.split(\"/\")[-1] + '/train', transform=transform)\n",
    "train_loader = DataLoader(train_set, batch_size=256, shuffle=True, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = torchvision.datasets.ImageFolder(root='../../data/classifier/' + in_dir.split(\"/\")[-1] + '/test', transform=transform)\n",
    "test_loader = DataLoader(test_set, batch_size=256, shuffle=True, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)  # 27 total classes - text captcha have 0 so it is removed for now\n",
    "model = model.to(device)\n",
    "model = nn.DataParallel(model)  # Use DataParallel\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    for i, data in progress_bar:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)  # Move data to the primary device\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=running_loss / (i + 1))\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}\")\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can be saved directly from the GPU\n",
    "torch.save(model, './pth/test_ep25.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To load the model later\n",
    "# model = torch.load('./pth/far_shah_b1-b3_rn50_ep25.pth')\n",
    "# model.eval()  # Set it to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Variables to hold predictions and actual labels\n",
    "y_pred = []\n",
    "y_true = []\n",
    "y_score = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        probabilities = torch.softmax(outputs, dim=1)  # Assuming outputs are raw scores from your model\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        # Accumulate true labels and predictions\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_score.extend(probabilities.cpu().numpy())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_by_class = []\n",
    "for i in range (0, num_classes):\n",
    "    tmp = []\n",
    "    for j, gt in enumerate(y_true):\n",
    "        if i == gt:\n",
    "            tmp.append([gt, y_pred[j]])\n",
    "        \n",
    "    gt_by_class.append(tmp)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = 0 \n",
    "for i, class_ in enumerate(gt_by_class):\n",
    "    class_cntr = 1\n",
    "    for inst in class_:\n",
    "        if inst[0] == inst[1]:\n",
    "            class_cntr += 1\n",
    "    print(i, class_list[i], class_cntr)\n",
    "    tot = tot + class_cntr / len(class_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot/len(gt_by_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert accumulated predictions and labels to numpy arrays\n",
    "y_pred = np.array(y_pred)\n",
    "y_true = np.array(y_true)\n",
    "y_score = np.array(y_score)\n",
    "\n",
    "# Determine the unique classes in y_true and binarize\n",
    "classes = np.unique(y_true)  # Identify unique class labels\n",
    "y_true_binarized = label_binarize(y_true, classes=classes)\n",
    "\n",
    "n_classes = len(classes)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = np.mean(y_pred == y_true)\n",
    "precision = precision_score(y_true, y_pred, average=\"weighted\", labels=classes)\n",
    "recall = recall_score(y_true, y_pred, average=\"weighted\", labels=classes)\n",
    "f1 = f1_score(y_true, y_pred, average=\"weighted\", labels=classes)\n",
    "\n",
    "# ROC Curve and AUC for Micro-average\n",
    "fpr, tpr, _ = roc_curve(y_true_binarized.ravel(), y_score.ravel())\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Print metrics\n",
    "print(\"OVERALL METRICS\")\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')\n",
    "\n",
    "# Calculate accuracy by class\n",
    "accuracies_by_class = {}\n",
    "for cls in classes:\n",
    "    idx = np.where(y_true == cls)[0]\n",
    "    accuracy_cls = accuracy_score(y_true[idx], y_pred[idx])\n",
    "    accuracies_by_class[cls] = accuracy_cls\n",
    "\n",
    "# Print accuracy by class\n",
    "print()\n",
    "print(\"ACCURACY BY CLASS\")\n",
    "for cls, acc in accuracies_by_class.items():\n",
    "    print(f'{class_list[cls]} (class {cls}): {acc * 100:.2f}%')\n",
    "\n",
    "# Calculate and visualize the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "plt.figure(figsize=(20, 12))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC Curve for Micro-average\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=lw, label='Micro-average ROC curve (area = {0:0.2f})'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic - Micro-average')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_class_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
