{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import clip\n",
    "# from transformers import CLIPProcessor, CLIPModel\n",
    "import os\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla P40'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda_device_id = 3\n",
    "torch.cuda.set_device(3)\n",
    "torch.cuda.get_device_name(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load('/home/pyt_user/pp/pytorch/clip/checkpoints/ckpt_e0.pth')\n",
    "# model = torch.load('/mnt/nis_lab_research/data/clip_data/pth/far_shah_b1-b5_b8_train_neg_vitb32_ep30/model_final.pth')\n",
    "model = clip.load('RN50x64')[0]\n",
    "device = torch.device(f\"cuda:{cuda_device_id}\")\n",
    "# device = \"cpu\"\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "preprocess = clip.load(\"RN50x64\", device=device, jit=False)[1]\n",
    "# processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sbert = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "model_sbert = model_sbert.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_5 (similarity):\n",
    "    top5 = []\n",
    "    values, indices = similarity[0].topk(5)  \n",
    "    for value, index in zip(values, indices):\n",
    "        top5.append([labels[index], 100 * value.item()])\n",
    "        \n",
    "    return top5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_clip_inference(image, context, labels):\n",
    "    \n",
    "    image_tensor = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "    context_tensor = clip.tokenize(context[:77]).to(device)\n",
    "    labels_tensor = torch.cat([clip.tokenize(txt.lower()) for txt in labels]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image_tensor)\n",
    "        context_features = model.encode_text(context_tensor)\n",
    "        label_features = model.encode_text(labels_tensor)        \n",
    "          \n",
    "    # Get images, context, and label features\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    context_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    label_features /= label_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Calculate similarity score for image vs label\n",
    "    similarity_img = (100.0 * image_features @ label_features.T).softmax(dim=-1)\n",
    "    \n",
    "    # Calculate similarity score for image vs label\n",
    "    similarity_cont = (100.0 * context_features @ label_features.T).softmax(dim=-1)\n",
    "    \n",
    "    similarity_comb = similarity_img + similarity_cont\n",
    "    \n",
    "    return [similarity_img, similarity_cont, similarity_comb]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sbert_inference (context, labels):\n",
    "    context_embedding = model_sbert.encode(context)\n",
    "    label_embeddings = model_sbert.encode(labels)\n",
    "    similarity = util.pytorch_cos_sim(context_embedding, label_embeddings)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = \"/mnt/nis_lab_research/data/clip_data/test/test1\"\n",
    "res_out_path = \"/home/pyt_user/pp/pytorch/clip/res/test1_res_clip_rn50x64.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = sorted(os.listdir(in_dir))\n",
    "    \n",
    "for label in labels:\n",
    "    \n",
    "    sd_path = os.path.join(in_dir, label)\n",
    "    img_fn_list = sorted([fn for fn in os.listdir(sd_path) if fn.endswith('.png')])\n",
    "    txt_fn_list = sorted([fn for fn in os.listdir(sd_path) if fn.endswith('.txt')])\n",
    "    \n",
    "    clip_ctr = 0\n",
    "    sbert_ctr = 0\n",
    "    comb_ctr = 0\n",
    "\n",
    "    for i in range(len(img_fn_list)):\n",
    "        \n",
    "        image_path = os.path.join(sd_path, img_fn_list[i])\n",
    "        txt_path = os.path.join(sd_path, txt_fn_list[i])\n",
    "        \n",
    "        with open(txt_path, \"r\") as f:\n",
    "            context = f.read().lower()\n",
    "        \n",
    "        # Run inference\n",
    "        clip_similarity = run_clip_inference(img, context, labels)\n",
    "        sbert_similarity = run_sbert_inference(context, labels)\n",
    "        if context != \"\":\n",
    "            comb_similarity = clip_similarity[0].to(\"cpu\") + sbert_similarity.to(\"cpu\")\n",
    "        else:\n",
    "            comb_similarity = clip_similarity[0].to(\"cpu\")\n",
    "        \n",
    "        clip_sim_lab = get_top_5(clip_similarity[0])[0][0]\n",
    "        sbert_sim_lab = get_top_5(sbert_similarity)[0][0]\n",
    "        comb_sim_lab = get_top_5(comb_similarity)[0][0]\n",
    "        \n",
    "        if label == clip_sim_lab:\n",
    "            clip_ctr += 1\n",
    "            \n",
    "        if label == sbert_sim_lab:\n",
    "            sbert_ctr += 1\n",
    "            \n",
    "        if label == comb_sim_lab:\n",
    "            comb_ctr += 1\n",
    "            \n",
    "    with open(res_out_path, 'a+') as file:\n",
    "        file.write(f\"{label}\\n\")\n",
    "        file.write(f\"clip via img: {clip_ctr} {len(img_fn_list)} {clip_ctr / len(img_fn_list)}\\n\")\n",
    "        file.write(f\"sbert: {sbert_ctr} {len(img_fn_list)} {sbert_ctr / len(img_fn_list)}\\n\")\n",
    "        file.write(f\"comb: {comb_ctr} {len(img_fn_list)} {comb_ctr / len(img_fn_list)}\\n\\n\")\n",
    "\n",
    "    # print(label, \"processing\")\n",
    "    # print(\"clip via img:\", clip_ctr, len(img_fn_list), clip_ctr/len(img_fn_list))\n",
    "    # print(\"sbert:\", sbert_ctr, len(img_fn_list), sbert_ctr/len(img_fn_list))\n",
    "    # print(\"comb:\", comb_ctr, len(img_fn_list), comb_ctr/len(img_fn_list))\n",
    "        \n",
    "    # print()\n",
    "    # print(\"##################################################\")\n",
    "    # print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
