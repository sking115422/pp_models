{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "import os\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_clip.list_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-16', pretrained='datacomp_l_s1b_b8k')\n",
    "tokenizer = open_clip.get_tokenizer('ViT-B-16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = \"/mnt/nis_lab_research/data/clip_data/shah_b1_539_21\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = os.listdir(in_dir)\n",
    "acc_list = []\n",
    "tp_list = []\n",
    "tot_list = []\n",
    "\n",
    "for i, cat in enumerate(cats):\n",
    "    tp_ctr = 0\n",
    "    cat_pth = os.path.join(in_dir, cat)\n",
    "    print(cat)\n",
    "    img_list = [x for x in os.listdir(cat_pth) if x[-3:] == \"png\"]\n",
    "    for j, item in enumerate(img_list):\n",
    "        print(\"   Image Complete:\", j )\n",
    "        img_pth = os.path.join(cat_pth, item)\n",
    "        txt_pth = img_pth[0:-3] + \"txt\"\n",
    "        \n",
    "        with open(txt_pth, \"r\") as f:\n",
    "            cont = f.read()\n",
    "        \n",
    "        cat_emb = tokenizer(cats)\n",
    "        img_emb = preprocess(Image.open(img_pth)).unsqueeze(0)\n",
    "        cont_emb = tokenizer(cont)\n",
    "        \n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            cat_feat = model.encode_text(cat_emb)\n",
    "            img_feat = model.encode_image(img_emb)\n",
    "            cont_feat = model.encode_text(cont_emb)\n",
    "            \n",
    "            img_feat /= img_feat.norm(dim=-1, keepdim=True)\n",
    "            cat_feat /= cat_feat.norm(dim=-1, keepdim=True)\n",
    "            cont_feat /= cont_feat.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            cat_probs_img = (100.0 * img_feat @ cat_feat.T).softmax(dim=-1)[0].numpy().tolist()\n",
    "            lab_img = cats[cat_probs_img.index(max(cat_probs_img))]\n",
    "            \n",
    "            cat_probs_cont = (100.0 * cont_feat @ cat_feat.T).softmax(dim=-1)[0].numpy().tolist()\n",
    "            lab_cont = cats[cat_probs_cont.index(max(cat_probs_cont))]\n",
    "\n",
    "        lab_list = [lab_img, lab_cont]\n",
    "        max_probs = [max(cat_probs_img), max(cat_probs_cont)]\n",
    "        fin_lab = lab_list[max_probs.index(max(max_probs))]\n",
    "        \n",
    "        if fin_lab == cat:\n",
    "            tp_ctr += 1\n",
    "    \n",
    "    tp_list.append(tp_ctr)\n",
    "    tot_list.append(len(img_list))\n",
    "    acc_list.append(tp_ctr / len(img_list))\n",
    "    \n",
    "res_df = pd.DataFrame({\"Categories\":cats, \"True Postives\":tp_list, \"Tot Cat Instances\":tot_list, \"Accuracy\":acc_list})\n",
    "res_df.to_csv(\"./clip_results.csv\", index=False)\n",
    "    \n",
    "        \n",
    "        # print(\"   Label probs img:\", cat_probs_img)\n",
    "        # print(\"   Label img:\", lab_img, max(cat_probs_img))\n",
    "        # print(\"   Label probs cont:\", cat_probs_cont)\n",
    "        # print(\"   Label cont:\", lab_cont, max(cat_probs_cont))\n",
    "        # print()\n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    cat_feat = model.encode_text(cat_emb)\n",
    "    img_feat = model.encode_image(img_emb)\n",
    "    cont_feat = model.encode_text(cont_emb)\n",
    "    \n",
    "    img_feat /= img_feat.norm(dim=-1, keepdim=True)\n",
    "    cat_feat /= cat_feat.norm(dim=-1, keepdim=True)\n",
    "    cont_feat /= cont_feat.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    cat_probs_img = (100.0 * img_feat @ cat_feat.T).softmax(dim=-1)[0].numpy().tolist()\n",
    "    lab_img = categories[cat_probs_img.index(max(cat_probs_img))]\n",
    "    \n",
    "    cat_probs_cont = (100.0 * cont_feat @ cat_feat.T).softmax(dim=-1)[0].numpy().tolist()\n",
    "    lab_cont = categories[cat_probs_cont.index(max(cat_probs_cont))]\n",
    "    \n",
    "\n",
    "print(\"Label probs img:\", cat_probs_img)\n",
    "print(\"Label img:\", lab_img, max(cat_probs_img))\n",
    "\n",
    "print(\"Label probs cont:\", cat_probs_cont)\n",
    "print(\"Label cont:\", lab_cont, max(cat_probs_cont))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
